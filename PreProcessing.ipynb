{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter   # count the number of instances of each class\n",
    "import time      # time library to calculate the time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('train.csv')\n",
    "validation = pd.read_csv('validation.csv')\n",
    "test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in the dataset\n",
      "No duplicate values in the dataset\n",
      "LoanID column has been removed\n"
     ]
    }
   ],
   "source": [
    "#####Preprocessing (Train dataset)#####\n",
    "# check for missing values and drop if have\n",
    "if train.isnull().sum().sum() > 0:\n",
    "    train = train.dropna()\n",
    "    print(train.isnull().sum())\n",
    "else: \n",
    "    print(\"No missing values in the dataset\")\n",
    "\n",
    "\n",
    "#check for duplicate values and drop if have\n",
    "if train.duplicated().sum() > 0:\n",
    "    train = train.drop_duplicates()\n",
    "    print(train.duplicated().sum())\n",
    "else:\n",
    "    print(\"No duplicate values in the dataset\")\n",
    "\n",
    "\n",
    "#drop the loanID column as it is not required\n",
    "if 'LoanID' in train.columns:\n",
    "    train = train.drop(['LoanID'], axis=1)\n",
    "    print(\"LoanID column has been removed\")\n",
    "else:\n",
    "    print('LoanID column is not present or has removed, check the columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done converting categorical variables to numerical for train data\n",
      "Done Z-Score the data for train data\n"
     ]
    }
   ],
   "source": [
    "#converting CATEGORICAL variable to numerical/binary FOR TRAIN DATA\n",
    "train['Education'] = train['Education'].map({'High School':1,'Bachelor\\'s':2, 'Master\\'s':3, 'PhD':4}).astype(int)\n",
    "train['EmploymentType'] = train['EmploymentType'].map({'Unemployed':1,'Part-time':2, 'Full-time':3, 'Self-employed':4}).astype(int)\n",
    "train['MaritalStatus']= train['MaritalStatus'].map({'Single':1,'Married':2, 'Divorced':3}).astype(int)\n",
    "train['HasMortgage']= train['HasMortgage'].map({'No':0,'Yes':1}).astype(int)\n",
    "train['HasDependents']= train['HasDependents'].map({'No':0,'Yes':1}).astype(int)\n",
    "train['LoanPurpose']= train['LoanPurpose'].map({'Education':1,'Home':2, 'Business':3, 'Auto':4, 'Other':5}).astype(int)\n",
    "train['HasCoSigner']= train['HasCoSigner'].map({'No':0,'Yes':1}).astype(int)\n",
    "\n",
    "#convert the loan terms from months to years, reducing the scale\n",
    "train['LoanTerm']= (train['LoanTerm']/12).astype(int)\n",
    "\n",
    "print(\"Done converting categorical variables to numerical for train data\")\n",
    "\n",
    "\n",
    "#normalizing the data using Z-Score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_independent_variables = ['Age','Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \\\n",
    "                                'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "for column in train_independent_variables:\n",
    "    train[column] = scaler.fit_transform(train[[column]])\n",
    "    train[column] = train[column].apply(lambda x: round(x, 4))\n",
    "\n",
    "print(\"Done Z-Score the data for train data\")\n",
    "\n",
    "#one hot encoding for categorical variables\n",
    "columns_encode = ['Education', 'EmploymentType', 'MaritalStatus', 'LoanPurpose']\n",
    "train = pd.get_dummies(train, columns=columns_encode)\n",
    "\n",
    "#check and convert features boolean type to 1 and 0\n",
    "train_columns =train.columns\n",
    "for column in train_columns:\n",
    "    if train[column].dtype == bool:\n",
    "        train[column] = train[column].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 135408, 1: 17788})\n",
      "Counter({0: 135408, 1: 135408})\n",
      "Done resampling for train data\n",
      "No duplicate values in the dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# library for oversampling, undersampling and combination of both, pick one of them\n",
    "# Documentation: https://imbalanced-learn.org/dev/references/over_sampling.html\n",
    "\n",
    "# Note: balancing the data for the minority class \"Default=1\"\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks, EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTEENN       #SMOTEENN is a combination of SMOTE and Edited Nearest Neighbours(ENN).\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy='minority',random_state=42)\n",
    "\n",
    "#separate the data into Xs and Y\n",
    "X_train = train.drop(['Default'], axis=1)\n",
    "Y_train = train['Default']\n",
    "\n",
    "\n",
    "print(Counter(Y_train))\n",
    "\n",
    "X_train_resampled, Y_train_resampled = sm.fit_resample(X_train, Y_train)\n",
    "train_resampled = pd.concat([X_train_resampled, Y_train_resampled], axis=1)\n",
    "\n",
    "print(Counter(Y_train_resampled))\n",
    "print(\"Done resampling for train data\")\n",
    "\n",
    "#separate the data into Xs and Y from resampled data\n",
    "X_train = X_train_resampled\n",
    "Y_train = Y_train_resampled \n",
    "\n",
    "\n",
    "#overwriting the train data with resampled data\n",
    "train = train_resampled\n",
    "\n",
    "\n",
    "\n",
    "#remove duplicate rows\n",
    "if train.duplicated().sum() > 0:\n",
    "    train = train.drop_duplicates()\n",
    "    print(train.duplicated().sum())\n",
    "else:\n",
    "    print(\"No duplicate values in the dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             feature       VIF\n",
      "0                Age  1.042137\n",
      "1             Income  1.018082\n",
      "2         LoanAmount  1.016647\n",
      "3        CreditScore  1.002426\n",
      "4     MonthsEmployed  1.014512\n",
      "5     NumCreditLines  1.001596\n",
      "6       InterestRate  1.026063\n",
      "7           LoanTerm  1.000445\n",
      "8           DTIRatio  1.000930\n",
      "9        HasMortgage  1.761855\n",
      "10     HasDependents  1.746044\n",
      "11       HasCoSigner  1.738945\n",
      "12       Education_1  2.715058\n",
      "13       Education_2  2.697545\n",
      "14       Education_3  2.622482\n",
      "15       Education_4  2.611281\n",
      "16  EmploymentType_1  2.794758\n",
      "17  EmploymentType_2  2.706543\n",
      "18  EmploymentType_3  2.575802\n",
      "19  EmploymentType_4  2.671660\n",
      "20   MaritalStatus_1  3.743785\n",
      "21   MaritalStatus_2  3.666009\n",
      "22   MaritalStatus_3  3.823498\n",
      "23     LoanPurpose_1  2.128395\n",
      "24     LoanPurpose_2  2.089041\n",
      "25     LoanPurpose_3  2.154885\n",
      "26     LoanPurpose_4  2.129116\n",
      "27     LoanPurpose_5  2.126086\n",
      "Index(['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed',\n",
      "       'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'HasMortgage',\n",
      "       'HasDependents', 'HasCoSigner', 'Education_1', 'Education_2',\n",
      "       'Education_3', 'Education_4', 'EmploymentType_1', 'EmploymentType_2',\n",
      "       'EmploymentType_3', 'EmploymentType_4', 'LoanPurpose_1',\n",
      "       'LoanPurpose_2', 'LoanPurpose_3', 'LoanPurpose_4', 'LoanPurpose_5'],\n",
      "      dtype='object')\n",
      "20    MaritalStatus_1 has been removed from the dataset\n",
      "21    MaritalStatus_2 has been removed from the dataset\n",
      "22    MaritalStatus_3 has been removed from the dataset\n",
      "Name: feature, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# feature reduction, why we need it? - to reduce the features/dimension of the model\n",
    "# VIF for each features in the train dataset\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(len(X_train.columns))]\n",
    "print(vif_data)\n",
    "\n",
    "#drop the columns if VIF > *your optimum value*\n",
    "VIF_VALUE = 3\n",
    "if vif_data['VIF'].max() > VIF_VALUE:\n",
    "    vif_data = vif_data[vif_data['VIF'] > VIF_VALUE]\n",
    "    X_train = X_train.drop(vif_data['feature'], axis=1)\n",
    "    print(X_train.columns)\n",
    "    print(vif_data['feature'] + \" has been removed from the dataset\") \n",
    "else: \n",
    "    print(X_train.columns)\n",
    "\n",
    "\n",
    "#combine the Xs and Ys back\n",
    "train = pd.concat([X_train, Y_train], axis=1)\n",
    "\n",
    "\n",
    "#export (train) to csv\n",
    "train.to_csv('train_preprocessed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed',\n",
      "       'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'HasMortgage',\n",
      "       'HasDependents', 'HasCoSigner', 'Education_1', 'Education_2',\n",
      "       'Education_3', 'Education_4', 'EmploymentType_1', 'EmploymentType_2',\n",
      "       'EmploymentType_3', 'EmploymentType_4', 'LoanPurpose_1',\n",
      "       'LoanPurpose_2', 'LoanPurpose_3', 'LoanPurpose_4', 'LoanPurpose_5'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# validation data\n",
    "if 'LoanID' in validation.columns:\n",
    "    validation = validation.drop(['LoanID'], axis=1)\n",
    "else:\n",
    "    print('LoanID column is not present or has removed, check the columns')\n",
    "\n",
    "\n",
    "#converting CATEGORICAL variable to numerical/binary FOR VALIDATION DATA\n",
    "validation['Education'] = validation['Education'].map({'High School':1,'Bachelor\\'s':2, 'Master\\'s':3, 'PhD':4}).astype(int)\n",
    "validation['EmploymentType'] = validation['EmploymentType'].map({'Unemployed':1,'Part-time':2, 'Full-time':3, 'Self-employed':4}).astype(int)\n",
    "validation['MaritalStatus']= validation['MaritalStatus'].map({'Single':1,'Married':2, 'Divorced':3}).astype(int)\n",
    "validation['HasMortgage']= validation['HasMortgage'].map({'No':0,'Yes':1}).astype(int)\n",
    "validation['HasDependents']= validation['HasDependents'].map({'No':0,'Yes':1}).astype(int)\n",
    "validation['LoanPurpose']= validation['LoanPurpose'].map({'Education':1,'Home':2, 'Business':3, 'Auto':4, 'Other':5}).astype(int)\n",
    "validation['HasCoSigner']= validation['HasCoSigner'].map({'No':0,'Yes':1}).astype(int)\n",
    "validation['LoanTerm']= (validation['LoanTerm']/12).astype(int)\n",
    "\n",
    "\n",
    "# List of columns to be processed for Z-score normalization\n",
    "val_independent_variables = ['Age','Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \\\n",
    "                            'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "# apply Z-score normalization and rounding to 4 decimal places\n",
    "for column in val_independent_variables:\n",
    "    validation[column] = scaler.fit_transform(validation[[column]])\n",
    "    validation[column] = validation[column].apply(lambda x: round(x, 4))\n",
    "\n",
    "\n",
    "# One hot encoding for categorical variables #columns_encode is at the train data\n",
    "validation = pd.get_dummies(validation, columns=columns_encode)\n",
    "\n",
    "# check and save the columns name that boolean values and convert them to binary\n",
    "validation_columns = validation.columns\n",
    "for column in validation_columns:\n",
    "    if validation[column].dtype == bool:\n",
    "        validation[column] = validation[column].astype(int)\n",
    "\n",
    "\n",
    "#split validation data into X and Y\n",
    "X_validation = validation.drop(['Default'], axis=1)\n",
    "Y_validation = validation['Default']\n",
    "\n",
    "\n",
    "\n",
    "#drop the independent variables based on VIF values from train\n",
    "X_validation = X_validation.drop(vif_data['feature'], axis=1)\n",
    "print(X_validation.columns)\n",
    "\n",
    "#combine the Xs and Ys back\n",
    "# export (validation) to csv \n",
    "validation = pd.concat([X_validation, Y_validation], axis=1)\n",
    "validation.to_csv('validation_preprocessed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed',\n",
      "       'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'HasMortgage',\n",
      "       'HasDependents', 'HasCoSigner', 'Education_1', 'Education_2',\n",
      "       'Education_3', 'Education_4', 'EmploymentType_1', 'EmploymentType_2',\n",
      "       'EmploymentType_3', 'EmploymentType_4', 'LoanPurpose_1',\n",
      "       'LoanPurpose_2', 'LoanPurpose_3', 'LoanPurpose_4', 'LoanPurpose_5'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#%%script echo skipping           # this line is to skip running this cell. Uncomment if you want to skip this cell\n",
    "#######Preprocessing test data #######\n",
    "\n",
    "if 'LoanID' in test.columns:\n",
    "    test = test.drop(['LoanID'], axis=1)\n",
    "else:\n",
    "    print('LoanID column is not present or has removed, check the columns')\n",
    "\n",
    "test['Education'] = test['Education'].map({'High School':1,'Bachelor\\'s':2, 'Master\\'s':3, 'PhD':4}).astype(int)\n",
    "test['EmploymentType'] = test['EmploymentType'].map({'Unemployed':1,'Part-time':2, 'Full-time':3, 'Self-employed':4}).astype(int)\n",
    "test['MaritalStatus']= test['MaritalStatus'].map({'Single':1,'Married':2, 'Divorced':3}).astype(int)\n",
    "test['HasMortgage']= test['HasMortgage'].map({'No':0,'Yes':1}).astype(int)\n",
    "test['HasDependents']= test['HasDependents'].map({'No':0,'Yes':1}).astype(int)\n",
    "test['LoanPurpose']= test['LoanPurpose'].map({'Education':1,'Home':2, 'Business':3, 'Auto':4, 'Other':5}).astype(int)\n",
    "test['HasCoSigner']= test['HasCoSigner'].map({'No':0,'Yes':1}).astype(int)\n",
    "test['LoanTerm']= (test['LoanTerm']/12).astype(int)\n",
    "\n",
    "# List of columns to be processed for Z-score normalization\n",
    "test_independent_variables = ['Age','Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', \\\n",
    "                       'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
    "\n",
    "# apply Z-score normalization and rounding to 4 decimal places\n",
    "for column in test_independent_variables:\n",
    "    test[column] = scaler.fit_transform(test[[column]])\n",
    "    test[column] = test[column].apply(lambda x: round(x, 4))\n",
    "\n",
    "\n",
    "# One hot encoding for categorical variables #columns_encode is at the train data\n",
    "test = pd.get_dummies(test, columns=columns_encode)\n",
    "\n",
    "\n",
    "#split validation data into X and Y\n",
    "X_test = test.drop(['Default'], axis=1)\n",
    "Y_test = test['Default']\n",
    "\n",
    "\n",
    "\n",
    "#drop the independent variables based on VIF values from train\n",
    "X_test = X_test.drop(vif_data['feature'], axis=1)\n",
    "print(X_test.columns)\n",
    "\n",
    "#combine the Xs and Ys back\n",
    "# export (test) to csv\n",
    "test = pd.concat([X_test, Y_test], axis=1)\n",
    "test.to_csv('test_preprocessed.csv', index=False) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
